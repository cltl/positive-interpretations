{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "The following code is used to prepare the positive interpretations dataset for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import utils.data_preparation as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read and convert CoNLL files\n",
    "\n",
    "First, we read the CoNLL-2011 annotated files and do the following:\n",
    "\n",
    "- get the original sentence identifiers (not per part, but per file)\n",
    "- merge all data into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and concatenate\n",
    "df_dev = utils.get_all_tokens_conll(\"../data/conll2011-gold/dev_gold_conll\" )\n",
    "df_test = utils.get_all_tokens_conll(\"../data/conll2011-gold/test_gold_conll\" )\n",
    "df_train = utils.get_all_tokens_conll(\"../data/conll2011-gold/train_gold_conll\" )\n",
    "df_conll = pd.concat([df_dev, df_test, df_train])\n",
    "df_conll = df_conll[df_train.columns] # keep original order of columns\n",
    "df_conll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file, or load df_conll\n",
    "#df_conll.to_csv(\"/Users/Chantal/Corpora/conll-2011/gold/all_gold_conll\", sep=\"\\t\", index=False)\n",
    "df_conll = pd.read_csv(\"../data/conll2011-gold/all_gold_conll\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convert annotations\n",
    "\n",
    "Then, we convert the original annotations file for the positive interpretations to a format that is easier to work with and that is extended with some additional information:\n",
    "\n",
    "- use tab as separator (instead of #)\n",
    "- split the verb & role information into separate columns\n",
    "- add original sentence identifiers of OntoNotes\n",
    "- convert scores into classes (tertiary and binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "ann_file = \"../data/NAACL2016-Annotations/Annotations-SemanticRoles.csv\"\n",
    "columns = [\"file_id\", \"part_id\", \"sent_id_part\", \"predicate\", \"verb\", \"role\", \"negation\", \n",
    "          \"positive_interpretation\", \"label\"]\n",
    "df = pd.read_csv(ann_file, sep=\"#\", quoting=csv.QUOTE_NONE, names=columns, index_col=False)\n",
    "df = df[df.label != \"invalid\"] # one invalid instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert and add information\n",
    "df = utils.find_original_sent_ids(df, df_conll)  \n",
    "df = utils.rewrite_verb_and_role_features(df)\n",
    "df = utils.categorize_scores(df)\n",
    "\n",
    "# Change order of columns\n",
    "columns = ['file_id','sent_id_file','part_id','sent_id_part','predicate','negation','positive_interpretation',\n",
    "           'verb_wf','verb_pos','verb_span','verb_label','verb_tokens',\n",
    "           'role_head_wf','role_head_pos','role_span','role_label','role_tokens',\n",
    "           'label', 'class_tertiary', 'class_binary']\n",
    "df = df[columns]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to new file\n",
    "tsv_file = \"../data/NAACL2016-Annotations/Annotations-SemanticRoles.tsv\"\n",
    "df.to_csv(tsv_file, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split into train/test sets\n",
    "\n",
    "Finally, we split the data into a train/test sets. **Note:** splitting is done randomly, so this will generate a new train/test split every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from file\n",
    "tsv_file = \"../data/NAACL2016-Annotations/Annotations-SemanticRoles.tsv\"\n",
    "df = pd.read_csv(tsv_file, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split random into test/train\n",
    "df_train, df_test = utils.split_train_test(df, test_ratio=0.2, to_shuffle=True)\n",
    "df_train[\"dataset\"] = \"train\"\n",
    "df_test[\"dataset\"] = \"test\"\n",
    "df_all = pd.concat([df_train, df_test])\n",
    "print(len(df_train), len(df_test), len(df_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write to files\n",
    "df_test.to_csv(\"../data/test.tsv\", sep=\"\\t\", index=False)\n",
    "df_train.to_csv(\"../data/train.tsv\", sep=\"\\t\", index=False)\n",
    "df_all.to_csv(\"../data/all.tsv\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
